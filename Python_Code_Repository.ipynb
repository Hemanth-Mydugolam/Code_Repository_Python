{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetching GDP country wise\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the HTML content of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the relevant table (the first table with class 'wikitable' in this case)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract table headers\n",
    "headers = []\n",
    "for th in table.find_all('th'):\n",
    "    headers.append(th.text.strip())\n",
    "\n",
    "# Step 5: Extract table rows\n",
    "rows = []\n",
    "for tr in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = tr.find_all('td')\n",
    "    row = [cell.text.strip() for cell in cells]\n",
    "    if len(row) > 0:  # Ensuring the row is not empty\n",
    "        rows.append(row)\n",
    "\n",
    "# ***IMP*** This is something we need to take care manully for each table        \n",
    "headers = ['Country','IMF_Estimate','IMF_Year','WB_Estimate','WB_Year','UN_Estimate','UN_Year']  \n",
    "\n",
    "# Step 6: Convert to DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Step 7: Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetching interest rates\n",
    "\n",
    "# Step 1: Fetch the HTML content of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_sovereign_states_by_central_bank_interest_rates\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the relevant table (the first table with class 'wikitable' in this case)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract table headers\n",
    "headers = []\n",
    "for th in table.find_all('th'):\n",
    "    headers.append(th.text.strip())\n",
    "\n",
    "# Step 5: Extract table rows\n",
    "rows = []\n",
    "for tr in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = tr.find_all('td')\n",
    "    row = [cell.text.strip() for cell in cells]\n",
    "    if len(row) > 0:  # Ensuring the row is not empty\n",
    "        rows.append(row)\n",
    "\n",
    "# Step 6: Convert to DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Step 7: Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
